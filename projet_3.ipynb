{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31141b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• T√©l√©chargement en cours...\n",
      "‚úÖ Nouveau fichier d√©tect√© et sauvegard√© : dataset\\dvf_20250702.csv.gz\n",
      "üîç Chargement du fichier dans un DataFrame...\n",
      "20,133,668 lignes charg√©es.\n",
      "Colonnes disponibles : ['id_mutation', 'date_mutation', 'numero_disposition', 'nature_mutation', 'valeur_fonciere', 'adresse_numero', 'adresse_suffixe', 'adresse_nom_voie', 'adresse_code_voie', 'code_postal', 'code_commune', 'nom_commune', 'code_departement', 'ancien_code_commune', 'ancien_nom_commune', 'id_parcelle', 'ancien_id_parcelle', 'numero_volume', 'lot1_numero', 'lot1_surface_carrez', 'lot2_numero', 'lot2_surface_carrez', 'lot3_numero', 'lot3_surface_carrez', 'lot4_numero', 'lot4_surface_carrez', 'lot5_numero', 'lot5_surface_carrez', 'nombre_lots', 'code_type_local', 'type_local', 'surface_reelle_bati', 'nombre_pieces_principales', 'code_nature_culture', 'nature_culture', 'code_nature_culture_speciale', 'nature_culture_speciale', 'surface_terrain', 'longitude', 'latitude']\n"
     ]
    }
   ],
   "source": [
    "import requests, hashlib, os\n",
    "from datetime import datetime\n",
    "\n",
    "# Dossier o√π tu stockes tes sources\n",
    "dossier = \"dataset\"\n",
    "os.makedirs(dossier, exist_ok=True)\n",
    "\n",
    "# Chemins utiles\n",
    "fichier_temp = os.path.join(dossier, \"temp_dvf.csv.gz\")\n",
    "fichier_hash = os.path.join(dossier, \"dernier_hash.txt\")\n",
    "\n",
    "# URL stable du fichier DVF g√©olocalis√©\n",
    "url = \"https://www.data.gouv.fr/fr/datasets/r/d7933994-2c66-4131-a4da-cf7cd18040a4\"\n",
    "\n",
    "# 1. T√©l√©charger temporairement\n",
    "print(\"üì• T√©l√©chargement en cours...\")\n",
    "r = requests.get(url, stream=True)\n",
    "with open(fichier_temp, 'wb') as f:\n",
    "    for chunk in r.iter_content(8192):\n",
    "        f.write(chunk)\n",
    "\n",
    "# 2. Calcul de l'empreinte SHA1\n",
    "def sha1(path):\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "nouveau_hash = sha1(fichier_temp)\n",
    "ancien_hash = open(fichier_hash).read().strip() if os.path.exists(fichier_hash) else None\n",
    "\n",
    "# 3. Comparaison & archivage\n",
    "if nouveau_hash != ancien_hash:\n",
    "    nom_final = os.path.join(dossier, f\"dvf_{datetime.today().strftime('%Y%m%d')}.csv.gz\")\n",
    "    os.rename(fichier_temp, nom_final)\n",
    "    with open(fichier_hash, 'w') as f:\n",
    "        f.write(nouveau_hash)\n",
    "    print(f\"‚úÖ Nouveau fichier d√©tect√© et sauvegard√© : {nom_final}\")\n",
    "else:\n",
    "    os.remove(fichier_temp)\n",
    "    print(\"‚úîÔ∏è Aucun changement d√©tect√©. Le fichier est inchang√©.\")\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "# Chargement direct dans un DataFrame\n",
    "print(\"üîç Chargement du fichier dans un DataFrame...\")\n",
    "df = pd.read_csv(nom_final, compression='gzip', low_memory=False)\n",
    "\n",
    "# Aper√ßu rapide\n",
    "print(f\"{len(df):,} lignes charg√©es.\")\n",
    "print(\"Colonnes disponibles :\", list(df.columns))\n",
    "\n",
    "# Suppression des colonnes adresse_suffixe, ancien_code_commune, ancien_nom_commune, ancien_id_parcelle, numero_volume, nature_culture_speciale, code_nature_culture\n",
    "df = df.drop(columns=['adresse_suffixe', 'ancien_code_commune', 'ancien_nom_commune', 'ancien_id_parcelle', 'numero_volume','nature_culture_speciale', 'code_nature_culture'])\n",
    "\n",
    "# Suppression des lignes o√π la valeur fonciere est nulle:\n",
    "df = df.dropna(subset=['valeur_fonciere'])\n",
    "\n",
    "# Suppression des lignes o√π la longitude et latitude sont nulles:\n",
    "df=df.dropna(subset=['longitude'])\n",
    "\n",
    "# Suppression des lignes missing de la colonne 'type_local' afin de garder uniquement maison, appartement, dependances et local:\n",
    "df = df.dropna(subset=['type_local'])\n",
    "\n",
    "# suppression des colonnes lot1_numero, lot1_surface_carrez, lot2_numero, lot2_surface_carrez, lot3_numero, lot3_surface_carrez, lot4_numero, lot4_surface_carrez, lot5_numero, lot5_surface_carrez, nombre_lots:\n",
    "df = df.drop(columns=['lot1_numero', 'lot1_surface_carrez','lot2_numero', 'lot2_surface_carrez', 'lot3_numero', 'lot3_surface_carrez', 'lot4_numero', 'lot4_surface_carrez', 'lot5_numero', 'lot5_surface_carrez', 'nombre_lots']) \n",
    "\n",
    "#suppression des type_local qui ne sont pas maison ou appartement: \n",
    "df_filtr√© = df[df[\"type_local\"].isin([\"Maison\", \"Appartement\"])]\n",
    "\n",
    "#suppression des nature mutations qui ne sont pas vente ou vente en l'etat futur d'achevement: \n",
    "df_filtr√© = df_filtr√©[df_filtr√©[\"nature_mutation\"].isin([\"Vente\", \"Vente en l'etat futur d'ach√®vement\"])]\n",
    "\n",
    "#suppression de la colonne code_nature_culture_speciale:\n",
    "df_filtr√© = df_filtr√©.drop(columns=['code_nature_culture_speciale'])\n",
    "\n",
    "#suppression des colonnes surface_terrain et nature_culture:\n",
    "df_filtr√© = df_filtr√©.drop(columns=['surface_terrain', 'nature_culture'])\n",
    "\n",
    "#suppression des lignes doublons:\n",
    "df_filtr√© = df_filtr√©.drop_duplicates()\n",
    "\n",
    "# Conservons uniquement une ligne unique par id_mutation avec somme du nombre de pieces et somme de la surface batie:\n",
    "\n",
    "df_agr√©g√© = df_filtr√©.groupby('id_mutation', as_index=False).agg({\n",
    "    'date_mutation': 'first',\n",
    "    'numero_disposition': 'first',\n",
    "    'nature_mutation': 'first',\n",
    "    'valeur_fonciere': 'first',\n",
    "    'adresse_numero': 'first',\n",
    "    'adresse_nom_voie': 'first',\n",
    "    'adresse_code_voie': 'first',\n",
    "    'code_postal': 'first',\n",
    "    'code_commune': 'first',\n",
    "    'nom_commune': 'first',\n",
    "    'code_departement': 'first',\n",
    "    'id_parcelle': 'first',\n",
    "    'code_type_local': 'first',\n",
    "    'surface_reelle_bati': 'sum',\n",
    "    'nombre_pieces_principales': 'sum',\n",
    "    'type_local': 'first',\n",
    "    'longitude': 'first',\n",
    "    'latitude': 'first',\n",
    "})\n",
    "\n",
    "# creation d'une nouvelle colonne prix_m2:\n",
    "df_agr√©g√©['prix_m2'] = df_agr√©g√©['valeur_fonciere'] / df_agr√©g√©['surface_reelle_bati']\n",
    "\n",
    "# Supprimer les lignes avec des prix_m2 infinis:\n",
    "\n",
    "df_agr√©g√©= df_agr√©g√©[~df_agr√©g√©['prix_m2'].isin([float('inf'), float('-inf')])]  # enl√®ve les inf\n",
    "\n",
    "# Conversion vers int apr√®s nettoyage\n",
    "df_agr√©g√©['prix_m2'] = df_agr√©g√©['prix_m2'].astype(int)\n",
    "\n",
    "#suppression des lignes pour lesquelles le prix_m2 est egal √† 0:\n",
    "df_agr√©g√© = df_agr√©g√©[df_agr√©g√©['prix_m2'] != 0]\n",
    "\n",
    "# transformation de la colonne date_mutation en type datetime:\n",
    "df_agr√©g√©['date_mutation'] = pd.to_datetime(df_agr√©g√©['date_mutation'])\n",
    "\n",
    "#sauvegarde en fichier csv:\n",
    "df_agr√©g√©.to_csv(os.path.join(dossier, 'df_agr√©g√©.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
